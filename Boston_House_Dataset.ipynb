{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noa Pereira Prada Schnor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning and Statistics Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GMIT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This assessment concerns the well-known Boston House Prices dataset and the Python packages scipy, keras, and jupyter. The project consists in five sections: 1. Introduction of the Boston House Price Dataset; 2. Description - descriptive statistics and plots to describe the Boston House Price dataset; 3. Inference/Analysis - using inferential statistics to analyse whether there is a significant difference in median house prices between houses that are along the Charles river and those that aren’t; 4. Prediction - using keras to create a neural network that can predict the median house price based on the other variables in the dataset and 5. Conclusions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Description of the Boston House Price Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset was first published in 1978 contains US census data concerning houses in various areas around the city of Boston. Each sample (row) corresponds to a unique area and has about 13 measures (variables/columns)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Descriptive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import scipy\n",
    "from scipy.stats import mstats\n",
    "from scipy.stats import kruskal\n",
    "import statsmodels.api as sm\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load the Boston Housing dataset from sklearn.datasets\n",
    "df = load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Description of the dataset\n",
    "df.DESCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shape - visualization of the length and the breadth of the dataset\n",
    "df.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Conversion to numpy array - target is the variable MEDV (input is the price and the outputs are the 'feature names')\n",
    "\n",
    "df_bsn = pd.DataFrame(df.data,columns=df.feature_names)\n",
    "df_bsn['target'] = pd.Series(df.target)\n",
    "\n",
    "df_bsn = pd.DataFrame(df['data'], columns=df['feature_names'])\n",
    "df_bsn['target'] = df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking missing data\n",
    "df_bsn.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the first rows\n",
    "df_bsn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Return a Numpy representation of the DataFrame - check if the dataset is stored in an array\n",
    "df_bsn.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summary and description of the dataset to get a detailed statistical information for each column\n",
    "\n",
    "df_bsn.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df_bsn, hue=\"CHAS\", markers=[\"o\", \"s\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df_bsn, hue=\"CHAS\", vars=[\"CRIM\", \"LSTAT\", \"B\", \"DIS\", \"ZN\", \"RM\", \"target\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the target (price) it looks like that the variable CRIM (per capita crime rate by town) does not seem to have a strong relationship with price regarding the area traits the River or not as most of the areas that the tract bounds the River have a value nearly 0 of CRIM and the price varies a lot. However, when the areas do not bounds the River it looks like the price tends to be higher when the CRIM value is low.\n",
    "Most of the areas that bounds the River have a value of B around 400 and it looks like the price varies a lot in areas with the same value of B.\n",
    "Mainly areas that bounds the River have a value of 0 for ZN (proportion of residential land zoned for lots over 25,000 sq.ft.) and even with the same value of ZN the price varies. \n",
    "The LSTAT (% lower status of the population) variable seems to be negatively correlated and the variable RM (average number of rooms per dwelling) positively correlated with price for CHAS equal to 1 or 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking for outliers\n",
    "\n",
    "sns.boxplot(x=\"CHAS\", y=\"target\", data=df_bsn)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable target (price) is skewed to the right. Furthermore, it looks like that there are some unusually high median-values in the data, especially in the areas that do not bounds the River. To confirm it there is a QQ Plot of those variables below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = sm.qqplot(df_bsn[\"target\"])\n",
    "\n",
    "dt = pr.findobj(lambda x: hasattr(x, 'get_color') and x.get_color() == 'b')\n",
    "\n",
    "[d.set_alpha(0.3) for d in dt]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The QQ-Plot shows that upper-third seems to come from a different distribution than the lower two-thirds.  Therefore, the median-values of price aren’t normally distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Split the input and the output variables\n",
    "\n",
    "prices = df_bsn['target']\n",
    "features = df_bsn.drop('target', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bsn.corr(method='pearson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap pf Pearson Correlation with no redundant mappings\n",
    "\n",
    "corr = df_bsn.corr(method='pearson')\n",
    "fig, ax = plt.subplots(figsize=(9, 9))\n",
    "mask = np.zeros_like(corr)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "with sns.axes_style(\"white\"):\n",
    "     ax = sns.heatmap(corr, annot=True, fmt=\".2f\", mask=mask)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the Pearson Correlation the attributes LSTAT, RM, and PTRATIO seem to have good correlation with the variable price. These variables maybe can be considered when optimizing the predictive value for price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "attributes = ['LSTAT', 'RM', 'PTRATIO','CHAS']\n",
    "price = df_bsn['target']\n",
    "\n",
    "for i, col in enumerate(attributes):\n",
    "    plt.subplot(1, len(attributes) , i+1)\n",
    "    x = df_bsn[col]\n",
    "    y = price\n",
    "    plt.scatter(x, y, c='g', marker='+')\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('House prices in $1K')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the raking of all attributes using linear regression as a model\n",
    "\n",
    "data = df[\"data\"]\n",
    "price = df[\"target\"]\n",
    "attributes = df[\"feature_names\"]\n",
    " \n",
    "linear = LinearRegression()\n",
    "\n",
    "\n",
    "rfe = RFE(linear, n_features_to_select=1)\n",
    "rfe.fit(data,price)\n",
    "sorted(zip(map(lambda x: round(x, 4), rfe.ranking_), attributes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check number of areas that the land tract bounds Charles River\n",
    "df_bsn['CHAS'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 35 neighborhoods on the Charles river."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Setting the variable CHAS as an integer\n",
    "df_bsn['CHAS'] = df_bsn['CHAS'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confirming the type and size of each attribute\n",
    "df_bsn.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Splitting the dataset in 2 groups related to CHAS variable\n",
    "river = df_bsn[df_bsn['CHAS'] == 1.0]\n",
    "noriver = df_bsn[df_bsn['CHAS'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "river.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noriver.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the samples river and noriver have the same distribution? To get the answer it was performed the Mann Whitney U test and the Kruskal Wallis H test using scipy functions. These statistical tests are the the nonparametric version of the (paired) Student t-test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mann Whitney U test or Wilcoxon-Mann Whitney test\n",
    "scipy.stats.mannwhitneyu (river,noriver)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test results show that the samples are likely drawn from samples with differing distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kruskal(river,noriver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "river.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the neighborhood has 1 for the variable CHAS the variables that have a strong correlation with price (target) are RM and LSTAT and a week correlation with ZN, NOX, AGE and B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noriver.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the neighborhood has 0 for the variable CHAS the variables that have a strong correlation with price (target) are RM and LSTAT, a moderate correlation with CRIM, INDUS, NOX, AGE, RAD, TAX and PTRATIO and a week correlation with ZN, DIS and B."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Neural Network using keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the dataset into input features (i) and the feature we wish to predict - price (p)\n",
    "\n",
    "i = df_bsn.iloc[:,0:13] #assign the first 13 columns of our array to a variable i\n",
    "p = df_bsn.iloc[:,13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Make sure that the scale of the input features are similar as some of the dataset features have different scale and it makes difficult to for the initialization of the neural network\n",
    "\n",
    "minmaxscaler = preprocessing.MinMaxScaler()\n",
    "i_scale = minmaxscaler.fit_transform(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into a training set, a validation set and a test set (in total 6 variables i_train, i_val, i_test, p_train,p_val and p_test)\n",
    "\n",
    "i_train, i_val_and_test, p_train, p_val_and_test = train_test_split(i_scale, p, test_size=0.3) #val_and_test size will be 30% of the overall dataset\n",
    "i_val, i_test, p_val, p_test = train_test_split(i_val_and_test, p_val_and_test, test_size=0.5) #val_ and _test split equally to the validation set and the test set\n",
    "\n",
    "#Check the shapes of the arrays\n",
    "i_train.shape, i_val.shape, i_test.shape, p_train.shape, p_val.shape, p_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Variable _train counts for 70% of full dataset, _val for 15% and _test for 15%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Setting up the architecture - first and second layer as a dense (fully-connected) layers with 64 neurons, ReLU activation and the input shape is 13 and the last layer layer is a dense layer with 1 neuron\n",
    "\n",
    "model = Sequential([Dense(64, kernel_initializer='normal', activation='relu', input_shape=(13,)),Dense(64, kernel_initializer='normal',activation='relu'), Dense(1, kernel_initializer='normal'),\n",
    "])\n",
    "model.compile(optimizer='rmsprop',loss='mse',metrics=['adam'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Estimate of the model’s performance\n",
    "kf = KFold(n_splits=10)\n",
    "rslts = cross_val_score(estimator, X, Y, cv=kf)\n",
    "rslts.mean()\n",
    "rslts.std()\n",
    "\n",
    "#mean squared error including the average and standard deviation (average variance) across all 10 folds of the cross validation evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Training on the data\n",
    "\n",
    "h = model.fit(i_train, p_train, batch_size=10, epochs=100, validation_data=(i_val, p_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Find the accuracy of the test set - the aim is to get the test accuracy anywhere between 80% to 95%\n",
    "\n",
    "model.evaluate(i_test, p_test)[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
